# cogs/llm.py
import discord
from discord.ext import commands
from utils.helpers import create_embed
from utils.llm_handler import MixtralHandler, GeminiHandler
import os

class LLMCog(commands.Cog):
    def __init__(self, bot):
        self.bot = bot
        
        # Initialize Mixtral handler for /chat command
        mixtral_api_key = os.getenv('HF_TOKEN')
        if not mixtral_api_key:
            raise ValueError("HF_TOKEN not found in environment variables")
        self.mixtral = MixtralHandler(mixtral_api_key)
        
        # Initialize Gemini handler for mentions
        gemini_api_key = os.getenv('GOOGLE_TOKEN')
        if not gemini_api_key:
            raise ValueError("GOOGLE_TOKEN not found in environment variables")
        self.gemini = GeminiHandler(gemini_api_key)

    @commands.Cog.listener()
    async def on_message(self, message):
        """Handle mentions using Gemini"""
        if message.author == self.bot.user:
            return
            
        if self.bot.user in message.mentions:
            content = message.content.replace(f'<@{self.bot.user.id}>', '').strip()
            if content:
                async with message.channel.typing():
                    response = await self.gemini.generate_response(
                        message.author.id,
                        content
                    )
                    
                    if len(response) > 2000:
                        chunks = [response[i:i+2000] for i in range(0, len(response), 2000)]
                        for i, chunk in enumerate(chunks):
                            if i == 0:
                                await message.reply(chunk)
                            else:
                                await message.channel.send(chunk)
                    else:
                        await message.reply(response)

    @commands.hybrid_command(
        name="chat",
        description="Chat with the rude bot powered by Mixtral AI"
    )
    async def chat(self, ctx, *, message: str):
        """Chat with Mixtral with conversation memory"""
        await ctx.defer()
        
        try:
            async with ctx.typing():
                response = await self.mixtral.generate_response(
                    ctx.author.id,
                    message
                )
                
                if len(response) > 2000:
                    chunks = [response[i:i+2000] for i in range(0, len(response), 2000)]
                    for chunk in chunks:
                        await ctx.send(chunk)
                else:
                    await ctx.send(response)
                    
        except Exception as e:
            await ctx.send("Brain.exe has crashed. Try again later.")

    @commands.hybrid_command(
        name="clear_chat",
        description="Clear your chat history with both AI models"
    )
    async def clear_chat(self, ctx):
        """Clear the conversation history for both models"""
        self.mixtral.clear_history(ctx.author.id)
        self.gemini.clear_history(ctx.author.id)
        await ctx.send("Chat history cleared for both models.")

async def setup(bot):
    await bot.add_cog(LLMCog(bot))